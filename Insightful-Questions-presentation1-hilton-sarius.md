For the Scaling laws for Nural Language models paper, which of the factors (model size, dataset size, compute) has the greatest impact on reducing loss when scaling up

For the Training Compute-Optimal Large Language Models paper, Why did the authors propose Chinchilla as a better model compared to Gopher or GPTâ€‘3?
