# MAI5301: Foundations of Large Language Models @UG

* Course Code: MAI5301
* Course Name: Foundations of Large Language Models
* Credits: 4
* Sessions: Tuesday & Thursday 5:00 pm â€“ 6:30 pm (PDT) / 9:00 pm â€“ 10:30 pm (GYD)
* Duration: 15 weeks
* Teaching Modality: Online

### Team

| Member | Role | Office Hours |
| :------ | :--- | :----------- |
| Dr. Christopher Clarke | Faculty | **By appointments only** |

### Communication
**ALL** communication regarding this course must be via GitHub Issues and email.
This includes questions, discussions, announcements, as well as private messages.

Presentation slides and paper summaries will be submitted via a pull request to this repository.

## Course Description
This course provides students with an in-depth understanding of generative AI through state-of-the-art language models, focusing on transformers and models like ChatGPT. Through comprehensive technical study, students will delve into algorithmic, systems-level, and theoretical aspects of these models, while gaining hands-on experience building LLM components from scratch.

We will start with foundational concepts of language models and transformers, then take a deep dive into the technical aspects of large language models. Our topics will include: transformer architecture, attention mechanisms, pre-training objectives, scaling laws, fine-tuning methods, alignment techniques, and safety considerations. We will cover LLM topics primarily from top conferences in machine learning and natural language processing.

Note that this course **focuses on both AI methods and implementation**. Students will gain theoretical understanding while building practical skills through progressive coding assignments that implement LLM components from scratch.

### Prerequisites
Students are expected to have:
* Strong programming background (Python preferred)
* Linear algebra and probability theory
* Basic understanding of neural networks
Having prior experience with deep learning frameworks (PyTorch/TensorFlow) is helpful but not required.

### Textbook
This course has no required textbook.
We will read recent papers from top venues including ACL, EMNLP, NAACL, ICML, NeurIPS, ICLR, and other conferences to understand current trends in large language models.

## Tentative Schedule and Reading List

*This is an evolving list and subject to changes due to the rapid pace of LLM developments.*

| Date | Topics/Readings | Presenter | Summary | Reviewer |
| :---- | :---- | :---- | :---- | :---- |
| Week 1 | **Introduction to Language Models and Transformers** | Dr. Clarke |  |  |
|  | [How to Read a Paper](http://svr-sk818-web.cl.cam.ac.uk/keshav/papers/07/paper-reading.pdf) (Required) |  |  |  |
|  | [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Required) |  |  |  |
|  | [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (Required) |  |  |  |
| Week 2 | **Fundamentals of Transformers and Research Paper Reading** | Dr. Clarke |  |  |
|  | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (Required) |  |  |  |
|  | [How to Give a Bad Talk](http://www.cs.berkeley.edu/~pattrsn/talks/BadTalk.pdf) (Required) |  |  |  |
|  | [Writing Reviews for Systems Conferences](http://people.inf.ethz.ch/troscoe/pubs/review-writing.pdf) (Required) |  |  |  |

*Weeks 3-15 will feature student-led presentations and will be scheduled based on paper selections and course progress.*

## Policies

### Honor Code
Academic integrity applies to all activities related to this course.

### Groups
All activities of this course will be performed **individually** due to small class size (4 students).

### Required Reading
Each lecture will have **required readings that everyone must read**.
There will be *optional related readings* that presenters should be familiar with.
They are optional for the rest of the class.

### Student Lectures
The course will be conducted as a seminar starting from Week 3.
Each student will present multiple papers over the course of the semester.
Presentations should cover all required papers for that lecture.
The duration of the presentation should be **at most 45 minutes** with **15 minutes for discussion**.
Presenters should expect questions and interruptions throughout.

In the presentation, you should:

* Provide necessary background and motivate the problem.
* Present the high level idea, approach, and/or insight (using examples, whenever appropriate) in the required reading as well as the additional reading.
* Discuss technical details so that one can understand key details without carefully reading.
* Explain the differences between related works.
* Identify strengths and weaknesses of the required reading and propose directions of future research.

*The slides for a presentation must be submitted via a pull request at least 24 hours prior to the corresponding class.*

### Lecture Summaries
Each student will be assigned to **write summaries for lectures they do not present**.
The student will write a summary for all presented papers (required readings) for that lecture.

A paper summary must address the following questions in sufficient details (1-2 pages):

* What is the problem addressed in the lecture, and why is this problem important?
* What is the state of related works in this topic?
* What is the proposed solution, and what key insight guides their solution?
* What is one (or more) drawback or limitation of the proposal?
* What are potential directions for future research?

*The paper summary must be submitted via a pull request within 24 hours after the presentation.*
**Late reviews will not be counted.**

*Allocate enough time for your reading, write the summary carefully, and include key observations from the class discussion.*

### Post-Presentation Panel Discussion
To foster a deeper understanding of the papers and encourage critical thinking, each lecture will be followed by a panel discussion.
This discussion will involve rotating roles among the students, simulating an interactive and dynamic scholarly exchange.

#### Roles and Responsibilities

1. **The Author**
   - Assignment: The presenting student acts as the paper's author
   - Responsibility: Defend the paper against critiques, answer questions, and discuss potential improvements or extensions, similar to writing a rebuttal during peer review

2. **The Reviewers**
   - Assignment: The other students serve as reviewers
   - Responsibility: Critically assess the papers, pose challenging questions, highlight potential weaknesses, and engage in constructive critique simulating peer review

3. **Audience Participation**
   - Responsibility: All students must submit **one insightful question** for each presented paper before each class and actively participate in the discussion

### Participation
Given the discussion-based nature of this course, participation is required both for your own understanding and to improve the overall quality of the course.
You are expected to attend **all** lectures and participate in class discussions.

### Coding Assignments
Progressive coding assignments will be distributed throughout the semester, designed to build LLM components from scratch and provide hands-on experience with transformer architecture and training.

ðŸ“Œ **Late Policy**: Coding assignments cannot be turned in late unless an extension is granted at least 24 hours before the submission deadline.

ðŸ“Œ **Implementation Requirements**: All assignments must be implemented from scratch using PyTorch, with clear documentation and explanations.

## Tentative Grading

| Component | Weight |
| --------- | ------:|
| Paper Presentations | 40% |
| Paper Summaries | 20% |
| Participation | 10% |
| Coding Assignments | 30% |

## Grading System

| Grade | Percentage |
|-------|------------|
| A | 80% - 100% |
| B | 70% - 79% |
| C | 60% - 69% |
| F | < 60% |

## Submission & Academic Integrity

â€¢ Submit assignments via GitHub by 11:59 PM (GYD)  
â€¢ **Plagiarism Policy**: You may not directly copy code or text without proper attribution  
â€¢ **Use of Generative AI**: Allowed as a learning aid for understanding concepts, but all submitted code must be your own implementation with proper documentation  

ðŸ“Œ **Honor Code**: Misuse of generative AI tools or plagiarism will be treated as an academic violation.

## Required Software & Tools

â€¢ **Python 3.8+** with packages: PyTorch, NumPy, Matplotlib  
â€¢ **Development Environment**: Jupyter Notebooks, VS Code (recommended)  
â€¢ **Version Control**: Git and GitHub  
â€¢ **Hardware**: GPU access recommended (Google Colab acceptable)  

## Course Resources

### Primary Materials
â€¢ Selected research papers from ICML, NeurIPS, ICLR, ACL, and other top-tier venues  
â€¢ Course GitHub repository with code templates and examples  

### Supplementary Resources
â€¢ Online tutorials and documentation for PyTorch and transformer implementations  
â€¢ Technical blogs and educational content on LLM development  
â€¢ Open-source transformer implementations for reference  

### Useful Websites
â€¢ [Hugging Face](https://huggingface.co/) - Pre-trained models and documentation  
â€¢ [Papers with Code](https://paperswithcode.com/) - Latest ML research with implementations  
â€¢ [Transformer Circuits](https://transformer-circuits.pub/) - Mechanistic interpretability research  
â€¢ [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - Implementation walkthrough  

## Communication

â€¢ **Course Announcements**: Via email and GitHub  
â€¢ **Questions**: Use GitHub Issues or email Dr. Clarke  
â€¢ **Office Hours**: By appointment  
â€¢ **Class Discussions**: Encouraged during and after sessions