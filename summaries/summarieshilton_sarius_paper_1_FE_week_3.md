Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research
The paper addresses the lack of transparency and reproducibility in language model (LM) pretraining data. Modern high‑performing LMs rarely disclose details of their training corpora, and even many open models do not release the datasets used to build them. This opacity makes it difficult to scientifically study how training data composition affects model behavior, capabilities, and limitations, hindering progress in understanding bias, robustness, scaling laws, and failures in LMs. An open, well‑documented pretraining corpus is therefore critical for advancing reproducible and principled LM research.
Prior work has relied on partially documented or inaccessible datasets such as Common Crawl‑based mixtures (e.g., used in GPT‑3 or LLaMA), or smaller curated corpora like C4. While some datasets are publicly available, they are often under‑documented, smaller in scale, or not representative of the data mixtures used in current frontier models. As a result, many findings about data filtering, deduplication, or mixing strategies remain either anecdotal or proprietary. 
The authors propose Dolma, a 3‑trillion‑token English corpus built from a diverse mixture of sources, including web text, scientific papers, code, public‑domain books, encyclopedic content, and social media. The key insight is that open, large‑scale, and meticulously documented data curation enables meaningful experimentation on how data decisions influence model quality. Beyond releasing the dataset, the authors provide detailed documentation of design principles and filtering choices, empirical analyses using intermediate versions of Dolma as well as an open‑source data curation toolkit enabling full reproducibility. Dolma has already been used to train OLMo, demonstrating its practical viability for state‑of‑the‑art LM research.
A key limitation is that Dolma is English‑only, restricting its applicability to multilingual modeling. Additionally, despite its scale, the corpus still reflects web‑derived biases and legal constraints, and reproducing experiments at this scale requires substantial computational resources, limiting accessibility for smaller research groups.
Future work may extend Dolma to multilingual or multimodal corpora, explore automated data quality evaluation, and study fine‑grained links between specific data sources and downstream behavioral traits. More broadly, Dolma opens the door to a science of data‑centric LM development grounded in transparency and reproducibility.
