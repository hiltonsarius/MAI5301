Paper 1
Training language models to follow instructions with human feedback

Training Language Models to Follow Instructions with Human Feedback (Ouyang et al., 2022) tackles a central limitation of large pre-trained language models: although models like GPT-3 exhibit strong capabilities on benchmarks, they are not inherently aligned with human intent and often generate unhelpful, toxic, or untruthful outputs when following user instructions. This misalignment is a barrier to real-world usefulness. Prior work improved model performance through instruction tuning and supervised fine-tuning, but these approaches do not directly optimize for human preferences or overall usefulness. The authors propose the InstructGPT approach, which fine-tunes a base language model using reinforcement learning from human feedback (RLHF), combining supervised fine-tuning on human-written demonstrations, training a reward model from human rankings of model outputs, and then applying a policy optimization algorithm to align the model with human preferences. A key insight is that RLHF enables much smaller models to outperform larger baseline models on human evaluations of helpfulness, truthfulness, and safety, demonstrating that alignment need not rely on sheer scale. However, the method has limitations: InstructGPT can still make simple errors, its performance depends on the quality and diversity of human feedback (which is costly to collect), and the original evaluations do not fully characterize how well the model generalizes to truly out-of-distribution or domain-shifted instructions. Future research could broaden and diversify human feedback data, explore alternative or more scalable reward learning techniques, enhance robustness and safety in high-stakes domains, and develop rigorous benchmarks for evaluating cross-domain and out-of-distribution instruction following.

Key observations from the class discussion:
ChatGpt was made free shortly after the release of this paper which suggests that the decision makers at OpenAI used the feedback loop from the free users to further improve their model
While the initial study indicated that ~40 humans did the labeling task, the process was most likely later outsourced to employees out of the USA to get more employees. However, later on, because of backlash from the out of US countries, employees were better compensated to do this work.


Paper 2
Scaling Instruction-Finetuned Language Models

Pretrained large language models (LLMs) such as PaLM and T5 have advanced natural language processing by enabling models to perform a range of tasks with minimal task-specific supervision, but they often require careful prompt design or few-shot examples to follow user instructions reliably. Instruction fine-tuning — training models on many tasks described as natural language instructions — has emerged as a way to improve generalization to unseen tasks and reduce reliance on prompt engineering, making models more useful in real-world interactions where users provide directives in natural language. This problem is important because modern language models are increasingly deployed in interfaces where users expect accurate, responsive behavior across a broad spectrum of tasks without bespoke prompt tuning for each new situation. 
Related Work:
Earlier work demonstrated that instruction tuning could significantly improve zero-shot and few-shot learning. For example, FLAN (from Wei et al., 2021) and T0 (Sanh et al., 2021) showed that fine-tuning on collections of tasks formatted as instructions boosts generalization compared to vanilla pretrained models. These studies revealed that the number of tasks, the diversity of language prompts, and the presence of instruction signals help models internalize broader patterns about how to respond to user requests. However, before this paper, research had not systematically quantified how scaling the number of instruction datasets, increasing model size, and including complex reasoning data (such as chain-of-thought examples) together influence performance across architectures and evaluation benchmarks. 
Proposed Solution and Insight:
The authors extend instruction fine-tuning by exploring three dimensions: (1) scaling the number of tasks used in instruction fine-tuning, (2) scaling model size, and (3) incorporating chain-of-thought (CoT) training data that includes reasoning traces. They construct instruction fine-tuning mixtures from hundreds to over 1,800 tasks covering a wide range of language understanding, reasoning, and generation problems. Models are evaluated on diverse benchmarks — including MMLU (standard multi-task language understanding), BBH (BigBench Hard), TyDiQA (multilingual QA), MGSM (multilingual math), and open-ended generation — across zero-shot, few-shot, and reasoning scenarios. The key insight is that instruction fine-tuning consistently enhances performance across model families (PaLM, T5, U-PaLM), and that increasing both the breadth of tasks and model capacity strengthens generalization to unseen tasks. For example, the 540B-parameter Flan-PaLM model fine-tuned on ~1.8K tasks achieves state-of-the-art results, significantly outperforming its non-fine-tuned counterpart across benchmarks and achieving around 75.2 % accuracy on five-shot MMLU. Smaller instruction-fine-tuned models like Flan-T5 also show strong performance relative to larger non-fine-tuned models, demonstrating the broad utility of instruction finetuning. 
Limitations:
Despite strong improvements, the approach has limitations. First, gains from adding more tasks exhibit diminishing returns beyond a certain point: while performance generally continues to improve with larger task mixtures, each additional task contributes smaller marginal benefits once core instructional patterns are well covered. This suggests that naive scaling of the number of tasks may become less efficient unless the added tasks bring truly novel instructional structure. Additionally, instruction fine-tuning relies on high-quality curated datasets; if instruction tasks do not reflect meaningful diversity or contain biases, the model may still struggle with domain-shifted tasks not well represented in training mixtures. Moreover, adding chain-of-thought examples improves reasoning, but collecting such data at scale can be expensive and task-specific. 
Future Research Directions:
Future work can explore better strategies for dataset selection and composition to maximize diversity and generalization per task, rather than simply maximizing quantity. Methods that automate construction of instruction datasets — for example by generating synthetic or self-supervised instructions — could reduce reliance on costly human annotation. Improving evaluation frameworks to measure robustness on truly out-of-distribution or specialized domain tasks will help assess where instruction fine-tuning still falls short. Research can also investigate modular or mixture-of-experts architectures that leverage specialized instruction components, and more efficient ways to capture reasoning patterns without large numbers of explicit CoT examples. Taken together, these directions aim to build more robust, flexible instruction-aligned models usable across diverse real-world scenarios. 

Key observations from the class discussion:
This paper is similar to how when a person watches a lot of movies of the same genre, like horror, the plot gets predictable. And in order to get an unexpected plot twist, the person will need to look at a movie that is of a totally different genre.
We spoke a bit about the diminishing returns observed when tasks added are not as diverse. And we made note about how we can draw parallel to the concept of diminishing marginal returns in macro economics. Then further highlighted that most of the developments done in AI can actually be mapped back to real life concepts
