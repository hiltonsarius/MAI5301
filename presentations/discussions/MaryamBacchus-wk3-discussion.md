While reading both papers, I noticed that they complemented each other well, with DeepMind's paper building on and correcting the work of OpenAI's paper.
OpenAI's approach: "For optimally compute-efficient training, most of the increase should go towards increased model size. A relatively small increase in data is needed to avoid reuse."
DeepMind's approach: Train longer on much more data. The model size and training tokens should grow proportionally. DeepMind proved this approach reaped better results - Chinchilla (70B parameters, 1.4T tokens) outperformed Gopher (280B parameters, 300B tokens) using the same compute budget.
What stood out to me was OpenAI's claim that error rates and performance rely primarily on the total parameter count, the dataset size, and the compute (PetaFLOP-days), while the specific architectural details like depth versus width had minimal effects. This goes against my preconceived notions about neural networks. I wonder if this finding is specific to the Transformer architecture (with attention, encoders, and decoders) or generalizes to all neural networks.